"""
å‘½ä»¤è¡Œæ¥å£

ä½¿ç”¨ typer æ„å»ºï¼Œæä¾›ï¼š
- å­å‘½ä»¤ç»“æ„ï¼ˆtranscribe, diarize, process, serveï¼‰
- ä¸°å¯Œçš„å¸®åŠ©ä¿¡æ¯
- è¿›åº¦æ˜¾ç¤º
- é…ç½®è¦†ç›–
"""

from pathlib import Path
from typing import Annotated, Optional

import typer
from rich.panel import Panel
from rich.table import Table

from .config import get_settings
from .logger import console, print_error, print_info, print_step, print_success, print_warning, setup_logging

# åˆ›å»ºä¸»åº”ç”¨
app = typer.Typer(
    name="meeting-ai",
    help="ğŸ™ï¸ Meeting AI - ç¦»çº¿ä¼šè®®çºªè¦å·¥å…·",
    no_args_is_help=True,
    rich_markup_mode="rich",
)


# ============================================================
# å›è°ƒå‡½æ•°ï¼ˆå…¨å±€é€‰é¡¹ï¼‰
# ============================================================
@app.callback()
def main(
    debug: Annotated[
        bool,
        typer.Option("--debug", "-d", help="å¯ç”¨è°ƒè¯•æ¨¡å¼"),
    ] = False,
    quiet: Annotated[
        bool,
        typer.Option("--quiet", "-q", help="å®‰é™æ¨¡å¼ï¼Œåªè¾“å‡ºç»“æœ"),
    ] = False,
) -> None:
    """
    ğŸ™ï¸ Meeting AI - æœ¬åœ°ç¦»çº¿ä¼šè®®çºªè¦å·¥å…·

    åŠŸèƒ½ï¼š
    - è¯­éŸ³è½¬å†™ï¼ˆASRï¼‰
    - è¯´è¯äººåˆ†ç¦»ï¼ˆDiarizationï¼‰
    - æ™ºèƒ½å‘½å
    - ä¼šè®®æ€»ç»“

    ä½¿ç”¨ç¤ºä¾‹ï¼š
        meeting-ai process audio.mp3
        meeting-ai transcribe audio.mp3 --model large-v3
        meeting-ai serve --port 8080
    """
    settings = get_settings()

    if debug:
        settings.debug = True
        settings.log_level = "DEBUG"

    if not quiet:
        setup_logging(
            level=settings.log_level,
            show_path=debug,
        )


# ============================================================
# info å‘½ä»¤
# ============================================================
@app.command()
def info() -> None:
    """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯å’Œé…ç½®"""
    settings = get_settings()

    # æ ‡é¢˜é¢æ¿
    console.print(
        Panel.fit(
            f"[bold blue]{settings.app_name}[/bold blue] v{settings.version}",
            subtitle="æœ¬åœ°ç¦»çº¿ä¼šè®®çºªè¦å·¥å…·",
        )
    )

    # é…ç½®è¡¨æ ¼
    table = Table(title="å½“å‰é…ç½®", show_header=True)
    table.add_column("é…ç½®é¡¹", style="cyan")
    table.add_column("å€¼", style="green")

    # è·¯å¾„é…ç½®
    table.add_row("æ•°æ®ç›®å½•", str(settings.paths.data_dir))
    table.add_row("æ¨¡å‹ç›®å½•", str(settings.paths.models_dir))
    table.add_row("è¾“å‡ºç›®å½•", str(settings.paths.output_dir))

    # ASR é…ç½®
    table.add_row("ASR æ¨¡å‹", settings.asr.model_name)
    table.add_row("ASR è®¾å¤‡", settings.asr.device)
    table.add_row("è®¡ç®—ç²¾åº¦", settings.asr.compute_type)

    # Diarization é…ç½®
    table.add_row("åˆ†ç¦»æ¨¡å‹", settings.diarization.model_name)
    hf_status = "âœ“ å·²é…ç½®" if settings.diarization.hf_token else "âœ— æœªé…ç½®"
    table.add_row("HF Token", hf_status)

    # LLM é…ç½®
    llm_status = "âœ“ å¯ç”¨" if settings.llm.enabled else "âœ— ç¦ç”¨"
    table.add_row("LLM å‘½å", llm_status)
    if settings.llm.model_path:
        table.add_row("LLM æ¨¡å‹", str(settings.llm.model_path))

    console.print(table)

    # æ£€æŸ¥ä¾èµ–
    console.print("\n[bold]ä¾èµ–æ£€æŸ¥:[/bold]")
    _check_dependencies()


def _check_dependencies() -> None:
    """æ£€æŸ¥å…³é”®ä¾èµ–æ˜¯å¦å¯ç”¨"""
    checks = [
        ("faster-whisper", "faster_whisper"),
        ("pyannote-audio", "pyannote.audio"),
        ("torch", "torch"),
        ("llama-cpp-python", "llama_cpp"),
    ]

    for name, module in checks:
        try:
            __import__(module)
            print_success(f"{name}")
        except ImportError:
            print_error(f"{name} [dim](æœªå®‰è£…)[/dim]")


# ============================================================
# transcribe å‘½ä»¤ï¼ˆé˜¶æ®µ 2ï¼‰
# ============================================================
@app.command()
def transcribe(
    audio_file: Annotated[
        Path,
        typer.Argument(
            help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„",
            exists=True,
            file_okay=True,
            dir_okay=False,
            readable=True,
        ),
    ],
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Whisper æ¨¡å‹ (tiny/base/small/medium/large-v3)"),
    ] = "small",
    language: Annotated[
        Optional[str],
        typer.Option("--language", "-l", help="æŒ‡å®šè¯­è¨€ï¼ˆå¦‚ zh, enï¼‰ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹"),
    ] = None,
    output: Annotated[
        Optional[Path],
        typer.Option("--output", "-o", help="è¾“å‡ºç›®å½•"),
    ] = None,
) -> None:
    """
    è½¬å†™éŸ³é¢‘æ–‡ä»¶

    å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡å­—ï¼ˆä¸åŒ…å«è¯´è¯äººä¿¡æ¯ï¼‰ã€‚

    ç¤ºä¾‹ï¼š
        meeting-ai transcribe meeting.mp3
        meeting-ai transcribe meeting.mp3 --model large-v3 --language zh
    """
    import json
    from datetime import datetime

    from rich.progress import Progress, SpinnerColumn, TextColumn

    from .services import get_asr_service
    from .utils.audio import ensure_wav_16k_mono, get_audio_info

    settings = get_settings()
    settings.ensure_dirs()

    # ä¸´æ—¶ä¿®æ”¹ ASR æ¨¡å‹è®¾ç½®
    settings.asr.model_name = model

    # ç¡®å®šè¾“å‡ºç›®å½•
    if output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = settings.paths.output_dir / f"{audio_file.stem}_{timestamp}"
    output.mkdir(parents=True, exist_ok=True)

    print_info(f"è¾“å…¥æ–‡ä»¶: {audio_file}")
    print_info(f"è¾“å‡ºç›®å½•: {output}")
    print_info(f"æ¨¡å‹: {model}")

    # è·å–éŸ³é¢‘ä¿¡æ¯
    audio_info = get_audio_info(audio_file)
    print_info(f"éŸ³é¢‘æ—¶é•¿: {audio_info['duration']:.1f} ç§’")

    # è½¬æ¢éŸ³é¢‘æ ¼å¼
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        progress.add_task("è½¬æ¢éŸ³é¢‘æ ¼å¼...", total=None)
        wav_path = ensure_wav_16k_mono(audio_file, output / "audio_16k.wav")

    # è¿è¡Œè½¬å†™
    print_step(1, 2, "åŠ è½½æ¨¡å‹...")
    service = get_asr_service()

    print_step(2, 2, "è½¬å†™éŸ³é¢‘...")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        progress.add_task("è½¬å†™ä¸­ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...", total=None)
        result = service.transcribe(wav_path, language=language)

    # ä¿å­˜ç»“æœ
    # 1. JSON æ ¼å¼
    (output / "transcript.json").write_text(
        result.model_dump_json(indent=2),
        encoding="utf-8",
    )

    # 2. çº¯æ–‡æœ¬
    (output / "transcript.txt").write_text(
        result.full_text,
        encoding="utf-8",
    )

    # 3. å¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬
    lines = []
    for seg in result.segments:
        lines.append(f"[{seg.format_time()}] {seg.text}")
    (output / "transcript_with_time.txt").write_text(
        "\n".join(lines),
        encoding="utf-8",
    )

    # æ˜¾ç¤ºç»“æœ
    print_success("è½¬å†™å®Œæˆï¼")
    console.print()
    print_info(f"è¯­è¨€: {result.language} (ç½®ä¿¡åº¦: {result.language_probability:.2%})")
    print_info(f"ç‰‡æ®µæ•°: {len(result.segments)}")
    print_info(f"ç»“æœå·²ä¿å­˜åˆ°: {output}")


# ============================================================
# diarize å‘½ä»¤ï¼ˆé˜¶æ®µ 1-2ï¼‰
# ============================================================
@app.command()
def diarize(
    audio_file: Annotated[
        Path,
        typer.Argument(
            help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„",
            exists=True,
            file_okay=True,
            dir_okay=False,
            readable=True,
        ),
    ],
    output: Annotated[
        Optional[Path],
        typer.Option("--output", "-o", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputs/<æ–‡ä»¶å>/ï¼‰"),
    ] = None,
    min_speakers: Annotated[
        Optional[int],
        typer.Option("--min-speakers", help="æœ€å°‘è¯´è¯äººæ•°"),
    ] = None,
    max_speakers: Annotated[
        Optional[int],
        typer.Option("--max-speakers", help="æœ€å¤šè¯´è¯äººæ•°"),
    ] = None,
) -> None:
    """
    è¯´è¯äººåˆ†ç¦»

    åˆ†æéŸ³é¢‘ä¸­çš„ä¸åŒè¯´è¯äººï¼Œè¾“å‡ºæ¯ä¸ªäººè¯´è¯çš„æ—¶é—´æ®µã€‚

    ç¤ºä¾‹ï¼š
        meeting-ai diarize meeting.mp3
        meeting-ai diarize meeting.wav --max-speakers 3
    """
    import json
    from datetime import datetime

    from rich.progress import Progress, SpinnerColumn, TextColumn
    from rich.table import Table

    from .services import get_diarization_service
    from .utils.audio import ensure_wav_16k_mono, get_audio_info

    settings = get_settings()
    settings.ensure_dirs()

    # ç¡®å®šè¾“å‡ºç›®å½•
    if output is None:
        # é»˜è®¤: outputs/<æ–‡ä»¶å_æ—¶é—´æˆ³>/
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = settings.paths.output_dir / f"{audio_file.stem}_{timestamp}"
    output.mkdir(parents=True, exist_ok=True)

    print_info(f"è¾“å…¥æ–‡ä»¶: {audio_file}")
    print_info(f"è¾“å‡ºç›®å½•: {output}")

    # è·å–éŸ³é¢‘ä¿¡æ¯
    audio_info = get_audio_info(audio_file)
    print_info(f"éŸ³é¢‘æ—¶é•¿: {audio_info['duration']:.1f} ç§’")

    # è½¬æ¢éŸ³é¢‘æ ¼å¼ï¼ˆpyannote éœ€è¦ 16kHz å•å£°é“ï¼‰
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        progress.add_task("è½¬æ¢éŸ³é¢‘æ ¼å¼...", total=None)
        wav_path = ensure_wav_16k_mono(audio_file, output / "audio_16k.wav")

    # è¿è¡Œè¯´è¯äººåˆ†ç¦»
    print_step(1, 2, "åŠ è½½æ¨¡å‹...")
    service = get_diarization_service()

    print_step(2, 2, "åˆ†æè¯´è¯äºº...")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        progress.add_task("è¯´è¯äººåˆ†ç¦»ä¸­ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...", total=None)
        result = service.diarize(
            wav_path,
            min_speakers=min_speakers,
            max_speakers=max_speakers,
        )

    # ä¿å­˜ç»“æœ
    # 1. ä¿å­˜å®Œæ•´çš„ JSON
    result_json = output / "diarization.json"
    result_json.write_text(
        result.model_dump_json(indent=2),
        encoding="utf-8",
    )

    # 2. ä¿å­˜æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
    result_txt = output / "diarization.txt"
    lines = [
        f"è¯´è¯äººåˆ†ç¦»ç»“æœ",
        f"=" * 40,
        f"éŸ³é¢‘æ–‡ä»¶: {audio_file.name}",
        f"æ£€æµ‹åˆ° {result.speaker_count} ä¸ªè¯´è¯äºº",
        f"å…± {len(result.segments)} ä¸ªç‰‡æ®µ",
        f"",
        f"æ—¶é—´è½´:",
        f"-" * 40,
    ]
    for seg in result.segments:
        lines.append(f"[{seg.format_time()}] {seg.speaker}")
    
    lines.extend([
        f"",
        f"è¯´è¯äººç»Ÿè®¡:",
        f"-" * 40,
    ])
    for speaker_id, info in result.speakers.items():
        lines.append(
            f"{speaker_id}: {info.segment_count} ä¸ªç‰‡æ®µ, "
            f"å…± {info.total_duration:.1f} ç§’"
        )
    
    result_txt.write_text("\n".join(lines), encoding="utf-8")

    # æ˜¾ç¤ºç»“æœ
    print_success(f"è¯´è¯äººåˆ†ç¦»å®Œæˆï¼")
    console.print()

    # æ˜¾ç¤ºè¯´è¯äººç»Ÿè®¡è¡¨æ ¼
    table = Table(title="è¯´è¯äººç»Ÿè®¡")
    table.add_column("è¯´è¯äºº", style="cyan")
    table.add_column("ç‰‡æ®µæ•°", justify="right")
    table.add_column("æ€»æ—¶é•¿", justify="right")
    table.add_column("å æ¯”", justify="right")

    total_duration = sum(s.total_duration for s in result.speakers.values())
    for speaker_id, info in sorted(result.speakers.items()):
        percentage = (info.total_duration / total_duration * 100) if total_duration > 0 else 0
        table.add_row(
            speaker_id,
            str(info.segment_count),
            f"{info.total_duration:.1f}s",
            f"{percentage:.1f}%",
        )

    console.print(table)
    console.print()
    print_info(f"ç»“æœå·²ä¿å­˜åˆ°: {output}")


# ============================================================
# process å‘½ä»¤ï¼ˆå®Œæ•´æµç¨‹ï¼‰
# ============================================================
@app.command()
def process(
    audio_file: Annotated[
        Path,
        typer.Argument(
            help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„",
            exists=True,
            file_okay=True,
            dir_okay=False,
            readable=True,
        ),
    ],
    output: Annotated[
        Optional[Path],
        typer.Option("--output", "-o", help="è¾“å‡ºç›®å½•"),
    ] = None,
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Whisper æ¨¡å‹"),
    ] = "small",
    language: Annotated[
        Optional[str],
        typer.Option("--language", "-l", help="æŒ‡å®šè¯­è¨€"),
    ] = None,
    no_diarize: Annotated[
        bool,
        typer.Option("--no-diarize", help="è·³è¿‡è¯´è¯äººåˆ†ç¦»"),
    ] = False,
    no_naming: Annotated[
        bool,
        typer.Option("--no-naming", help="è·³è¿‡æ™ºèƒ½å‘½å"),
    ] = False,
    no_correct: Annotated[
        bool,
        typer.Option("--no-correct", help="è·³è¿‡é”™åˆ«å­—æ ¡æ­£"),
    ] = False,
    no_summary: Annotated[
        bool,
        typer.Option("--no-summary", help="è·³è¿‡ä¼šè®®æ€»ç»“"),
    ] = False,
    enhance: Annotated[
        bool,
        typer.Option("--enhance", "-e", help="å¯ç”¨éŸ³é¢‘å¢å¼ºï¼ˆé™å™ª+å½’ä¸€åŒ–ï¼‰"),
    ] = False,
    deep_enhance: Annotated[
        bool,
        typer.Option("--deep-enhance", help="æ·±åº¦éŸ³é¢‘å¢å¼ºï¼ˆæ•ˆæœæ›´å¥½ä½†æ›´æ…¢ï¼‰"),
    ] = False,
    separate_voice: Annotated[
        bool,
        typer.Option("--separate-voice", help="åˆ†ç¦»äººå£°ï¼ˆå»é™¤èƒŒæ™¯éŸ³ä¹/éŸ³æ•ˆï¼‰"),
    ] = False,
    max_speakers: Annotated[
        Optional[int],
        typer.Option("--max-speakers", help="æœ€å¤šè¯´è¯äººæ•°"),
    ] = None,
) -> None:
    """
    å®Œæ•´å¤„ç†æµç¨‹

    æ‰§è¡Œå®Œæ•´çš„ä¼šè®®çºªè¦ç”Ÿæˆæµç¨‹ï¼š
    0. éŸ³é¢‘å¢å¼ºï¼ˆå¯é€‰ï¼‰- é™å™ªã€å»æ··å“ã€å½’ä¸€åŒ–
    1. è¯´è¯äººåˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
    2. è¯­éŸ³è½¬å†™
    3. é”™åˆ«å­—æ ¡æ­£ï¼ˆå¯é€‰ï¼‰
    4. å¯¹é½è¯´è¯äººå’Œæ–‡å­—
    5. æ™ºèƒ½å‘½åï¼ˆå¯é€‰ï¼‰
    6. ä¼šè®®æ€»ç»“ï¼ˆå¯é€‰ï¼‰

    ç¤ºä¾‹ï¼š
        meeting-ai process meeting.mp3
        meeting-ai process meeting.mp3 --model large-v3 --max-speakers 3
        meeting-ai process meeting.mp3 --no-diarize  # åªè½¬å†™ï¼Œä¸åˆ†ç¦»è¯´è¯äºº
        meeting-ai process meeting.mp3 --no-naming   # è·³è¿‡æ™ºèƒ½å‘½å
        meeting-ai process meeting.mp3 --no-correct  # è·³è¿‡é”™åˆ«å­—æ ¡æ­£
        meeting-ai process meeting.mp3 --no-summary  # è·³è¿‡ä¼šè®®æ€»ç»“
        meeting-ai process meeting.mp3 --enhance     # å¯ç”¨éŸ³é¢‘å¢å¼º
        meeting-ai process meeting.mp3 --deep-enhance  # æ·±åº¦å¢å¼ºï¼ˆæ›´æ…¢ä½†æ›´å¥½ï¼‰
        meeting-ai process meeting.mp3 --separate-voice  # åˆ†ç¦»äººå£°ï¼ˆé€‚åˆæœ‰èƒŒæ™¯éŸ³ä¹çš„è§†é¢‘ï¼‰
    """
    import json
    from datetime import datetime

    from rich.progress import Progress, SpinnerColumn, TextColumn
    from rich.table import Table

    from .services import (
        align_transcript_with_speakers,
        correct_segments,
        detect_all_genders,
        fix_unknown_speakers,
        format_summary_markdown,
        get_asr_service,
        get_diarization_service,
        get_naming_service,
        merge_adjacent_segments,
        summarize_meeting,
    )
    from .models import DiarizationResult, SpeakerInfo
    from .utils.audio import ensure_wav_16k_mono, get_audio_info

    settings = get_settings()
    settings.ensure_dirs()
    settings.asr.model_name = model

    # è®¡ç®—æ€»æ­¥éª¤æ•°
    total_steps = 2  # è½¬æ¢ + è½¬å†™
    if enhance or deep_enhance or separate_voice:
        total_steps += 1  # å¢å¼º
    if not no_correct and settings.llm.enabled:
        total_steps += 1  # æ ¡æ­£
    if not no_diarize:
        total_steps += 2  # åˆ†ç¦» + å¯¹é½
    if not no_diarize and not no_naming:
        total_steps += 1  # å‘½å
    if not no_summary and settings.llm.enabled:
        total_steps += 1  # æ€»ç»“
    current_step = 0

    # ç¡®å®šè¾“å‡ºç›®å½•
    if output is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = settings.paths.output_dir / f"{audio_file.stem}_{timestamp}"
    output.mkdir(parents=True, exist_ok=True)

    print_info(f"è¾“å…¥æ–‡ä»¶: {audio_file}")
    print_info(f"è¾“å‡ºç›®å½•: {output}")

    # è·å–éŸ³é¢‘ä¿¡æ¯
    audio_info = get_audio_info(audio_file)
    print_info(f"éŸ³é¢‘æ—¶é•¿: {audio_info['duration']:.1f} ç§’")

    # Step 1: è½¬æ¢éŸ³é¢‘æ ¼å¼
    console.print()
    current_step += 1
    print_step(current_step, total_steps, "è½¬æ¢éŸ³é¢‘æ ¼å¼...")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        progress.add_task("è½¬æ¢ä¸­...", total=None)
        wav_path = ensure_wav_16k_mono(audio_file, output / "audio_16k.wav")

    # Step 1.5: éŸ³é¢‘å¢å¼ºï¼ˆå¯é€‰ï¼‰
    if enhance or deep_enhance or separate_voice:
        current_step += 1
        print_step(current_step, total_steps, "éŸ³é¢‘å¢å¼º...")
        
        try:
            from .utils.enhance import enhance_audio
            
            enhanced_path = output / "audio_enhanced.wav"
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console,
            ) as progress:
                task_desc = []
                if separate_voice:
                    task_desc.append("åˆ†ç¦»äººå£°")
                if deep_enhance:
                    task_desc.append("æ·±åº¦é™å™ª")
                elif enhance:
                    task_desc.append("é™å™ª")
                task_desc.append("å½’ä¸€åŒ–")
                progress.add_task(f"å¤„ç†ä¸­ï¼ˆ{'+'.join(task_desc)}ï¼‰...", total=None)
                
                wav_path = enhance_audio(
                    wav_path,
                    enhanced_path,
                    denoise=enhance or deep_enhance,
                    normalize=True,
                    separate_voice=separate_voice,
                    deep_denoise=deep_enhance,
                )
            print_info("éŸ³é¢‘å¢å¼ºå®Œæˆ")
        except ImportError as e:
            print_warning(f"éŸ³é¢‘å¢å¼ºéœ€è¦é¢å¤–ä¾èµ–: {e}")
            print_warning("è¯·è¿è¡Œ: pip install noisereduce")
            print_warning("å¦‚éœ€æ·±åº¦é™å™ª: pip install deepfilternet")
            print_warning("å¦‚éœ€äººå£°åˆ†ç¦»: pip install demucs")

    # Step 2: è¯´è¯äººåˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
    diarization_result = None
    if not no_diarize:
        current_step += 1
        print_step(current_step, total_steps, "è¯´è¯äººåˆ†ç¦»...")
        diar_service = get_diarization_service()
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            progress.add_task("åˆ†ç¦»ä¸­ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...", total=None)
            diarization_result = diar_service.diarize(
                wav_path,
                max_speakers=max_speakers,
            )
        print_info(f"æ£€æµ‹åˆ° {diarization_result.speaker_count} ä¸ªè¯´è¯äºº")

    # Step 3: è¯­éŸ³è½¬å†™
    current_step += 1
    print_step(current_step, total_steps, "è¯­éŸ³è½¬å†™...")
    asr_service = get_asr_service()
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        progress.add_task("è½¬å†™ä¸­ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...", total=None)
        transcript_result = asr_service.transcribe(wav_path, language=language)
    print_info(f"è¯­è¨€: {transcript_result.language}, ç‰‡æ®µæ•°: {len(transcript_result.segments)}")

    # Step 3.5: é”™åˆ«å­—æ ¡æ­£ï¼ˆå¯é€‰ï¼‰
    if not no_correct and settings.llm.enabled:
        current_step += 1
        print_step(current_step, total_steps, "é”™åˆ«å­—æ ¡æ­£...")
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            progress.add_task("æ ¡æ­£ä¸­...", total=None)
            corrected_segments = correct_segments(transcript_result.segments)
            # æ›´æ–° transcript_result çš„ç‰‡æ®µ
            transcript_result.segments = corrected_segments
        print_info("æ–‡æœ¬æ ¡æ­£å®Œæˆ")

    # Step 4: å¯¹é½ï¼ˆå¦‚æœæœ‰è¯´è¯äººåˆ†ç¦»ç»“æœï¼‰
    final_speakers = {}
    if diarization_result is not None:
        current_step += 1
        print_step(current_step, total_steps, "å¯¹é½è¯´è¯äºº...")
        aligned_segments = align_transcript_with_speakers(
            transcript_result,
            diarization_result,
        )
        # ä¿®å¤ UNKNOWN è¯´è¯äºº
        fixed_segments = fix_unknown_speakers(aligned_segments)
        # åˆå¹¶ç›¸é‚»åŒä¸€è¯´è¯äººçš„ç‰‡æ®µ
        final_segments = merge_adjacent_segments(fixed_segments)
        
        # Step 5: æ™ºèƒ½å‘½åï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if not no_naming:
            current_step += 1
            print_step(current_step, total_steps, "æ™ºèƒ½å‘½å...")
            
            # æ€§åˆ«æ£€æµ‹
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console,
            ) as progress:
                progress.add_task("åˆ†æè¯´è¯äºº...", total=None)
                gender_map = detect_all_genders(wav_path, final_segments)
            
            # LLM å‘½å
            naming_service = get_naming_service()
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console,
            ) as progress:
                progress.add_task("æ¨æ–­è¯´è¯äººèº«ä»½...", total=None)
                final_speakers = naming_service.name_speakers(final_segments, gender_map)
            
            # æ›´æ–°ç‰‡æ®µä¸­çš„è¯´è¯äººæ˜¾ç¤ºå
            speaker_id_to_name = {
                spk_id: info.display_name 
                for spk_id, info in final_speakers.items()
            }
        else:
            # ä¸å‘½åï¼Œåªç»Ÿè®¡
            speaker_stats: dict[str, dict] = {}
            for seg in final_segments:
                spk = seg.speaker or "UNKNOWN"
                if spk not in speaker_stats:
                    speaker_stats[spk] = {"total_duration": 0.0, "segment_count": 0}
                speaker_stats[spk]["total_duration"] += seg.duration
                speaker_stats[spk]["segment_count"] += 1
            
            for spk_id, stats in speaker_stats.items():
                final_speakers[spk_id] = SpeakerInfo(
                    id=spk_id,
                    display_name=spk_id,
                    total_duration=stats["total_duration"],
                    segment_count=stats["segment_count"],
                )
            speaker_id_to_name = {spk_id: spk_id for spk_id in final_speakers}
        
        final_diarization = DiarizationResult(
            speakers=final_speakers,
            segments=final_segments,
        )
    else:
        final_segments = transcript_result.segments
        final_diarization = None
        speaker_id_to_name = {}
        final_speakers = {}

    # Step 6: ä¼šè®®æ€»ç»“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    meeting_summary = None
    if not no_summary and settings.llm.enabled and final_diarization is not None:
        current_step += 1
        print_step(current_step, total_steps, "ç”Ÿæˆä¼šè®®æ€»ç»“...")
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            progress.add_task("æ€»ç»“ä¸­...", total=None)
            meeting_summary = summarize_meeting(
                final_segments,
                final_speakers,
                duration=audio_info['duration'],
            )
        if meeting_summary:
            print_info(f"ä¼šè®®ä¸»é¢˜: {meeting_summary.title}")

    # ä¿å­˜ç»“æœ
    console.print()
    print_info("ä¿å­˜ç»“æœ...")

    # 1. è½¬å†™ç»“æœ
    (output / "transcript.json").write_text(
        transcript_result.model_dump_json(indent=2),
        encoding="utf-8",
    )
    (output / "transcript.txt").write_text(
        transcript_result.full_text,
        encoding="utf-8",
    )

    # 2. æœ€ç»ˆå¯¹é½ç»“æœ
    if final_diarization is not None:
        (output / "result.json").write_text(
            final_diarization.model_dump_json(indent=2),
            encoding="utf-8",
        )
        
        # ç”Ÿæˆæ˜“è¯»çš„å¯¹è¯æ ¼å¼ï¼ˆä½¿ç”¨æ˜¾ç¤ºåï¼‰
        lines = []
        for seg in final_segments:
            speaker_id = seg.speaker or "UNKNOWN"
            display_name = speaker_id_to_name.get(speaker_id, speaker_id)
            lines.append(f"[{seg.format_time()}] {display_name}: {seg.text}")
        (output / "result.txt").write_text(
            "\n".join(lines),
            encoding="utf-8",
        )

    # 3. ä¼šè®®æ€»ç»“
    if meeting_summary is not None:
        # JSON æ ¼å¼
        (output / "summary.json").write_text(
            meeting_summary.model_dump_json(indent=2),
            encoding="utf-8",
        )
        # Markdown æ ¼å¼
        summary_md = format_summary_markdown(
            meeting_summary,
            final_speakers,
            duration=audio_info['duration'],
        )
        (output / "summary.md").write_text(summary_md, encoding="utf-8")

    # æ˜¾ç¤ºç»“æœ
    console.print()
    print_success("å¤„ç†å®Œæˆï¼")
    console.print()

    # æ˜¾ç¤ºç»Ÿè®¡è¡¨æ ¼
    if final_diarization is not None:
        table = Table(title="è¯´è¯äººç»Ÿè®¡")
        table.add_column("ID", style="dim")
        table.add_column("åç§°", style="cyan")
        table.add_column("ç‰‡æ®µæ•°", justify="right")
        table.add_column("æ€»æ—¶é•¿", justify="right")
        table.add_column("å æ¯”", justify="right")

        total_duration = sum(s.total_duration for s in final_diarization.speakers.values())
        for speaker_id, info in sorted(final_diarization.speakers.items()):
            percentage = (info.total_duration / total_duration * 100) if total_duration > 0 else 0
            table.add_row(
                speaker_id,
                info.display_name,
                str(info.segment_count),
                f"{info.total_duration:.1f}s",
                f"{percentage:.1f}%",
            )
        console.print(table)
        console.print()

    # æ˜¾ç¤ºå‰å‡ ä¸ªç‰‡æ®µé¢„è§ˆ
    console.print("[bold]å¯¹è¯é¢„è§ˆï¼ˆå‰5æ¡ï¼‰:[/bold]")
    for seg in final_segments[:5]:
        speaker_id = seg.speaker or "UNKNOWN"
        display_name = speaker_id_to_name.get(speaker_id, speaker_id)
        text_preview = seg.text[:50] + "..." if len(seg.text) > 50 else seg.text
        console.print(f"  [cyan]{display_name}[/cyan]: {text_preview}")

    # æ˜¾ç¤ºä¼šè®®æ€»ç»“é¢„è§ˆ
    if meeting_summary is not None:
        console.print()
        console.print("[bold]ä¼šè®®æ€»ç»“:[/bold]")
        console.print(f"  [yellow]ä¸»é¢˜:[/yellow] {meeting_summary.title}")
        if meeting_summary.summary:
            summary_preview = meeting_summary.summary[:100]
            if len(meeting_summary.summary) > 100:
                summary_preview += "..."
            console.print(f"  [yellow]æ‘˜è¦:[/yellow] {summary_preview}")
        if meeting_summary.key_points:
            console.print(f"  [yellow]è¦ç‚¹:[/yellow] {len(meeting_summary.key_points)} æ¡")

    console.print()
    print_info(f"ç»“æœå·²ä¿å­˜åˆ°: {output}")


# ============================================================
# serve å‘½ä»¤ï¼ˆé˜¶æ®µ 5ï¼‰
# ============================================================
@app.command()
def serve(
    host: Annotated[
        str,
        typer.Option("--host", "-h", help="æœåŠ¡åœ°å€"),
    ] = "127.0.0.1",
    port: Annotated[
        int,
        typer.Option("--port", "-p", help="æœåŠ¡ç«¯å£"),
    ] = 8000,
    reload: Annotated[
        bool,
        typer.Option("--reload", "-r", help="å¼€å‘æ¨¡å¼ï¼ˆè‡ªåŠ¨é‡è½½ï¼‰"),
    ] = False,
) -> None:
    """
    å¯åŠ¨ Web æœåŠ¡

    å¯åŠ¨æœ¬åœ° Web ç•Œé¢ï¼Œå¯é€šè¿‡æµè§ˆå™¨è®¿é—®ã€‚

    ç¤ºä¾‹ï¼š
        meeting-ai serve
        meeting-ai serve --port 8080 --reload
    """
    print_info(f"å¯åŠ¨æœåŠ¡: http://{host}:{port}")

    # TODO: é˜¶æ®µ 5 å®ç°
    print_error("æ­¤åŠŸèƒ½å°šæœªå®ç°ï¼Œå°†åœ¨é˜¶æ®µ 5 å®Œæˆ")
    raise typer.Exit(1)


# ============================================================
# å…¥å£ç‚¹
# ============================================================
def main_entry() -> None:
    """åŒ…å…¥å£ç‚¹"""
    app()


if __name__ == "__main__":
    main_entry()
